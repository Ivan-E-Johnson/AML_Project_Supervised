{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:08:39.909447408Z",
     "start_time": "2024-03-20T20:08:18.057338895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.2.0\n",
      "Numpy version: 1.26.3\n",
      "Pytorch version: 2.2.0+cu121\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: c33f1ba588ee00229a309000e888f9817b4f1934\n",
      "MONAI __file__: /home/iejohnson/.venv/AML/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: 0.4.13\n",
      "ITK version: 5.3.0\n",
      "Nibabel version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 10.2.0\n",
      "Tensorboard version: 2.16.2\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.66.2\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.8\n",
      "pandas version: 2.2.0\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning\n",
    "from monai.utils import set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, list_data_collate, decollate_batch, DataLoader\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    EnsureChannelFirst,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    EnsureChannelFirstD,\n",
    "    SpatialResampleD,\n",
    "    SpacingD,\n",
    "    ResizeD,\n",
    "    LoadImage,\n",
    "    RandFlipD,\n",
    "    RandRotate,\n",
    "    RandZoom,\n",
    "    ScaleIntensity,\n",
    "    ToTensor,\n",
    "    RandFlipd, DataStatsD,\n",
    ")\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    LoadImaged,\n",
    "    ScaleIntensityd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ToTensord,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "print_config()\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "\n",
    "def init_data_lists(base_data_path):\n",
    "    mask_paths = []\n",
    "    image_paths = []\n",
    "    for dir in base_data_path.iterdir():\n",
    "        if dir.is_dir():\n",
    "            image_paths.append(dir / f\"{dir.name}_prostate.nii.gz\")\n",
    "            mask_paths.append(dir / f\"{dir.name}_segmentation.nii.gz\")\n",
    "    return image_paths, mask_paths\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:23:42.363143491Z",
     "start_time": "2024-03-20T20:23:42.276207662Z"
    }
   },
   "id": "d522c5d0fe49c57b"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:23:42.539651373Z",
     "start_time": "2024-03-20T20:23:42.525381396Z"
    }
   },
   "id": "e525d999f5d7e289"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "class Net(pytorch_lightning.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._model = UNet(\n",
    "            spatial_dims=3,\n",
    "            in_channels=1,\n",
    "            out_channels=1,\n",
    "            channels=(16, 32, 64, 128, 256), # Number of features in each layer\n",
    "            strides=(2, 2, 2, 2),\n",
    "            num_res_units=2,\n",
    "            norm=Norm.BATCH,\n",
    "        )\n",
    "        self.loss_function = DiceLoss(sigmoid=True)\n",
    "        # self.post_pred = Compose([EnsureType(\"tensor\", device=device), AsDiscrete(argmax=True, to_onehot=2)])\n",
    "        # self.post_label = Compose([EnsureType(\"tensor\", device=device), AsDiscrete(to_onehot=2)])\n",
    "        self.dice_metric = DiceMetric(include_background=False, reduction=\"mean\", get_not_nans=False)\n",
    "        self.best_val_dice = 0\n",
    "        self.best_val_epoch = 0\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "        self.prepare_data()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._model(x)\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # set up the correct data path\n",
    "\n",
    "        base_data_path = Path(\"/home/iejohnson/programing/Supervised_learning/DATA/SortedProstateData\")\n",
    "        image_paths, mask_paths = init_data_lists(base_data_path)\n",
    "        train_image_paths, test_image_paths, train_mask_paths, test_mask_paths = train_test_split(image_paths, mask_paths, test_size=0.2)\n",
    "        # Prepare data\n",
    "        train_files = [\n",
    "            {\"image\": img_path, \"label\": mask_path}\n",
    "            for img_path, mask_path in zip(train_image_paths, train_mask_paths)\n",
    "        ]\n",
    "        val_files = [\n",
    "            {\"image\": img_path, \"label\": mask_path}\n",
    "            for img_path, mask_path in zip(test_image_paths, test_mask_paths)\n",
    "        ]\n",
    "\n",
    "        train_files = train_files[:10]\n",
    "        val_files = val_files[:1]\n",
    "        # set the data transforms\n",
    "        RandFlipd_prob = .35\n",
    "        Spacing_dim = (1.5, 1.5, 3.0)\n",
    "        Size_dim = (96,96, 16)\n",
    "        ScaleIntensity_Image = (0, 255)\n",
    "        ScaleIntensity_Mask = (0, 1)\n",
    "        # set deterministic training for reproducibility\n",
    "        set_determinism(seed=0)\n",
    "\n",
    "        # define the data transforms\n",
    "        self.train_transforms = Compose(\n",
    "                [\n",
    "                    LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                    EnsureChannelFirstD(keys=[\"image\", \"label\"]), # Add channel to image and mask so\n",
    "                    # DataStatsD(keys=[\"image\", \"label\"]),\n",
    "                    SpacingD(keys=[\"image\", \"label\"], pixdim=Spacing_dim, mode=(\"bilinear\", \"nearest\")), # Downsample to 2mm spacing\n",
    "                    ResizeD(keys=[\"image\", \"label\"], spatial_size=Size_dim, mode=(\"bilinear\", \"nearest\")),\n",
    "                    ScaleIntensityd(keys=[\"image\"], minv=ScaleIntensity_Image[0], maxv=ScaleIntensity_Image[1]),\n",
    "                    ScaleIntensityd(keys=[\"label\"], minv=ScaleIntensity_Mask[0], maxv=ScaleIntensity_Mask[1]), # Coarse Segmentation combine all mask\n",
    "                    RandFlipd(keys=[\"image\", \"label\"], prob=RandFlipd_prob, spatial_axis=0),\n",
    "                    RandFlipd(keys=[\"image\", \"label\"], prob=RandFlipd_prob, spatial_axis=1),\n",
    "                    RandFlipd(keys=[\"image\", \"label\"], prob=RandFlipd_prob, spatial_axis=2),\n",
    "                    # DataStatsD(keys=[\"image\", \"label\"]),\n",
    "                    \n",
    "                ]\n",
    "            )\n",
    "        self.validation_transforms = Compose(\n",
    "            [\n",
    "                LoadImaged(keys=[\"image\", \"label\"]),\n",
    "                EnsureChannelFirstD(keys=[\"image\", \"label\"]), # Add channel to image and mask so \n",
    "                SpacingD(keys=[\"image\", \"label\"], pixdim=Spacing_dim, mode=(\"bilinear\", \"nearest\")), # Downsample to 2mm spacing\n",
    "                ResizeD(keys=[\"image\", \"label\"], spatial_size=Size_dim, mode=(\"bilinear\", \"nearest\")), \n",
    "                # DataStatsD(keys=[\"image\", \"label\"]),\n",
    "                ScaleIntensityd(keys=[\"image\"], minv=ScaleIntensity_Image[0], maxv=ScaleIntensity_Image[1]),\n",
    "                ScaleIntensityd(keys=[\"label\"], minv=ScaleIntensity_Mask[0], maxv=ScaleIntensity_Mask[1]), # Coarse Segmentation combine all mask\n",
    "                # DataStatsD(keys=[\"image\", \"label\"]),\n",
    "               \n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # we use cached datasets - these are 10x faster than regular datasets\n",
    "        self.train_ds = CacheDataset(\n",
    "            data=train_files,\n",
    "            transform=self.train_transforms,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=4,\n",
    "        )\n",
    "        self.val_ds = CacheDataset(\n",
    "            data=val_files,\n",
    "            transform=self.validation_transforms,\n",
    "            cache_rate=1.0,\n",
    "            num_workers=4,\n",
    "        )\n",
    "\n",
    "    #         self.train_ds = monai.data.Dataset(\n",
    "    #             data=train_files, transform=train_transforms)\n",
    "    #         self.val_ds = monai.data.Dataset(\n",
    "    #             data=val_files, transform=val_transforms)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_loader = DataLoader(\n",
    "            self.train_ds,\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=4,\n",
    "            collate_fn=list_data_collate,\n",
    "        )\n",
    "        return train_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        val_loader = DataLoader(self.val_ds, batch_size=1, num_workers=4)\n",
    "        return val_loader\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self._model.parameters(), 1e-4)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        output = self.forward(images)\n",
    "        print(output.shape)\n",
    "        loss = self.loss_function(output, labels)\n",
    "        tensorboard_logs = {\"train_loss\": loss.item()}\n",
    "        return {\"loss\": loss, \"log\": tensorboard_logs}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch[\"image\"], batch[\"label\"]\n",
    "        print(images.shape)\n",
    "        print(labels.shape)\n",
    "        roi_size = (96, 96, 24)\n",
    "        # outputs = sliding_window_inference(images,roi_size,1, self.forward)\n",
    "        outputs = self.forward(images)\n",
    "        print(outputs.shape)\n",
    "        loss = self.loss_function(outputs, labels)\n",
    "        outputs = [self.post_pred(i) for i in decollate_batch(outputs)]\n",
    "        labels = [self.post_label(i) for i in decollate_batch(labels)]\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        d = {\"val_loss\": loss, \"val_number\": len(outputs)}\n",
    "        self.validation_step_outputs.append(d)\n",
    "        return d\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_loss, num_items = 0, 0\n",
    "        for output in self.validation_step_outputs:\n",
    "            val_loss += output[\"val_loss\"].sum().item()\n",
    "            num_items += output[\"val_number\"]\n",
    "        mean_val_dice = self.dice_metric.aggregate().item()\n",
    "        self.dice_metric.reset()\n",
    "        mean_val_loss = torch.tensor(val_loss / num_items)\n",
    "        tensorboard_logs = {\n",
    "            \"val_dice\": mean_val_dice,\n",
    "            \"val_loss\": mean_val_loss,\n",
    "        }\n",
    "        if mean_val_dice > self.best_val_dice:\n",
    "            self.best_val_dice = mean_val_dice\n",
    "            self.best_val_epoch = self.current_epoch\n",
    "        print(\n",
    "            f\"current epoch: {self.current_epoch} \"\n",
    "            f\"current mean dice: {mean_val_dice:.4f}\"\n",
    "            f\"\\nbest mean dice: {self.best_val_dice:.4f} \"\n",
    "            f\"at epoch: {self.best_val_epoch}\"\n",
    "        )\n",
    "        self.validation_step_outputs.clear()  # free memory\n",
    "        return {\"log\": tensorboard_logs}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:38:08.700533523Z",
     "start_time": "2024-03-20T20:38:08.641669110Z"
    }
   },
   "id": "8f0cb007c5332242"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "788a3cb7ff6d63e5"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:23:43.108567216Z",
     "start_time": "2024-03-20T20:23:43.097281035Z"
    }
   },
   "id": "9ef25bb056117a6c"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset:   0%|          | 0/10 [00:00<?, ?it/s]\u001B[A\n",
      "Loading dataset:  10%|█         | 1/10 [00:00<00:01,  7.29it/s]\u001B[A\n",
      "Loading dataset:  20%|██        | 2/10 [00:00<00:01,  6.02it/s]\u001B[A\n",
      "Loading dataset:  30%|███       | 3/10 [00:00<00:01,  5.53it/s]\u001B[A\n",
      "Loading dataset:  50%|█████     | 5/10 [00:01<00:01,  4.46it/s]\u001B[A\n",
      "Loading dataset: 100%|██████████| 10/10 [00:01<00:00,  8.54it/s][A\n",
      "\n",
      "Loading dataset:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Loading dataset: 100%|██████████| 1/1 [00:00<00:00,  7.65it/s]\u001B[A\n",
      "Trainer will use only 1 of 2 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=2)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "Loading dataset:   0%|          | 0/10 [00:00<?, ?it/s]\u001B[A\n",
      "Loading dataset:  10%|█         | 1/10 [00:00<00:01,  4.79it/s]\u001B[A\n",
      "Loading dataset:  20%|██        | 2/10 [00:00<00:02,  2.94it/s]\u001B[A\n",
      "Loading dataset:  50%|█████     | 5/10 [00:00<00:00,  5.77it/s]\u001B[A\n",
      "Loading dataset: 100%|██████████| 10/10 [00:01<00:00,  8.93it/s][A\n",
      "\n",
      "Loading dataset:   0%|          | 0/1 [00:00<?, ?it/s]\u001B[A\n",
      "Loading dataset: 100%|██████████| 1/1 [00:00<00:00,  7.84it/s]\u001B[A\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name          | Type     | Params\n",
      "-------------------------------------------\n",
      "0 | _model        | UNet     | 4.8 M \n",
      "1 | loss_function | DiceLoss | 0     \n",
      "-------------------------------------------\n",
      "4.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.8 M     Total params\n",
      "19.232    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]torch.Size([1, 1, 96, 96, 16])\n",
      "torch.Size([1, 1, 96, 96, 16])\n",
      "torch.Size([1, 1, 96, 96, 16])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Net' object has no attribute 'post_pred'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 18\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# initialise Lightning's trainer.\u001B[39;00m\n\u001B[1;32m     10\u001B[0m trainer \u001B[38;5;241m=\u001B[39m pytorch_lightning\u001B[38;5;241m.\u001B[39mTrainer(\n\u001B[1;32m     11\u001B[0m     max_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m600\u001B[39m,\n\u001B[1;32m     12\u001B[0m     logger\u001B[38;5;241m=\u001B[39mtb_logger,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m     log_every_n_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, \n\u001B[1;32m     16\u001B[0m )\n\u001B[0;32m---> 18\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:544\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    542\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstatus \u001B[38;5;241m=\u001B[39m TrainerStatus\u001B[38;5;241m.\u001B[39mRUNNING\n\u001B[1;32m    543\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m--> 544\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    546\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:44\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     43\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mlauncher\u001B[38;5;241m.\u001B[39mlaunch(trainer_fn, \u001B[38;5;241m*\u001B[39margs, trainer\u001B[38;5;241m=\u001B[39mtrainer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m---> 44\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n\u001B[1;32m     47\u001B[0m     _call_teardown_hook(trainer)\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:580\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    573\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    574\u001B[0m ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_select_ckpt_path(\n\u001B[1;32m    575\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    576\u001B[0m     ckpt_path,\n\u001B[1;32m    577\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    578\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    579\u001B[0m )\n\u001B[0;32m--> 580\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n\u001B[1;32m    583\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:987\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m    982\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_signal_connector\u001B[38;5;241m.\u001B[39mregister_signal_handlers()\n\u001B[1;32m    984\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    985\u001B[0m \u001B[38;5;66;03m# RUN THE TRAINER\u001B[39;00m\n\u001B[1;32m    986\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[0;32m--> 987\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    989\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    990\u001B[0m \u001B[38;5;66;03m# POST-Training CLEAN UP\u001B[39;00m\n\u001B[1;32m    991\u001B[0m \u001B[38;5;66;03m# ----------------------------\u001B[39;00m\n\u001B[1;32m    992\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1031\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1029\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining:\n\u001B[1;32m   1030\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[0;32m-> 1031\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1032\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mset_detect_anomaly(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_detect_anomaly):\n\u001B[1;32m   1033\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit_loop\u001B[38;5;241m.\u001B[39mrun()\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1060\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1057\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_start\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1059\u001B[0m \u001B[38;5;66;03m# run eval step\u001B[39;00m\n\u001B[0;32m-> 1060\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1062\u001B[0m call\u001B[38;5;241m.\u001B[39m_call_callback_hooks(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mon_sanity_check_end\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1064\u001B[0m \u001B[38;5;66;03m# reset logger connector\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:182\u001B[0m, in \u001B[0;36m_no_grad_context.<locals>._decorator\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    180\u001B[0m     context_manager \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mno_grad\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m context_manager():\n\u001B[0;32m--> 182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mloop_run\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:135\u001B[0m, in \u001B[0;36m_EvaluationLoop.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mis_last_batch \u001B[38;5;241m=\u001B[39m data_fetcher\u001B[38;5;241m.\u001B[39mdone\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;66;03m# run step hooks\u001B[39;00m\n\u001B[0;32m--> 135\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_evaluation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataloader_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001B[39;00m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:396\u001B[0m, in \u001B[0;36m_EvaluationLoop._evaluation_step\u001B[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001B[0m\n\u001B[1;32m    390\u001B[0m hook_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_step\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mtesting \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    391\u001B[0m step_args \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    392\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001B[1;32m    393\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m using_dataloader_iter\n\u001B[1;32m    394\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m (dataloader_iter,)\n\u001B[1;32m    395\u001B[0m )\n\u001B[0;32m--> 396\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_strategy_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhook_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mstep_args\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_progress\u001B[38;5;241m.\u001B[39mincrement_processed()\n\u001B[1;32m    400\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m using_dataloader_iter:\n\u001B[1;32m    401\u001B[0m     \u001B[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001B[39;00m\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:309\u001B[0m, in \u001B[0;36m_call_strategy_hook\u001B[0;34m(trainer, hook_name, *args, **kwargs)\u001B[0m\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    308\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mprofile(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[Strategy]\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrainer\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhook_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 309\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[38;5;66;03m# restore current_fx when nested context\u001B[39;00m\n\u001B[1;32m    312\u001B[0m pl_module\u001B[38;5;241m.\u001B[39m_current_fx_name \u001B[38;5;241m=\u001B[39m prev_fx_name\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:412\u001B[0m, in \u001B[0;36mStrategy.validation_step\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module:\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_redirection(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalidation_step\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m--> 412\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[28], line 138\u001B[0m, in \u001B[0;36mNet.validation_step\u001B[0;34m(self, batch, batch_idx)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m    137\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_function(outputs, labels)\n\u001B[0;32m--> 138\u001B[0m outputs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_pred(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m decollate_batch(outputs)]\n\u001B[1;32m    139\u001B[0m labels \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_label(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m decollate_batch(labels)]\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdice_metric(y_pred\u001B[38;5;241m=\u001B[39moutputs, y\u001B[38;5;241m=\u001B[39mlabels)\n",
      "Cell \u001B[0;32mIn[28], line 138\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28mprint\u001B[39m(outputs\u001B[38;5;241m.\u001B[39mshape)\n\u001B[1;32m    137\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_function(outputs, labels)\n\u001B[0;32m--> 138\u001B[0m outputs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost_pred\u001B[49m(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m decollate_batch(outputs)]\n\u001B[1;32m    139\u001B[0m labels \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpost_label(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m decollate_batch(labels)]\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdice_metric(y_pred\u001B[38;5;241m=\u001B[39moutputs, y\u001B[38;5;241m=\u001B[39mlabels)\n",
      "File \u001B[0;32m~/.venv/AML/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m   1686\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[1;32m   1687\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[0;32m-> 1688\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'Net' object has no attribute 'post_pred'"
     ]
    }
   ],
   "source": [
    "# initialise the LightningModule\n",
    "net = Net()\n",
    "\n",
    "\n",
    "# set up loggers and checkpoints\n",
    "log_dir = os.path.join(\"/home/iejohnson/PycharmProjects/AML/AML_Project_Supervised\", \"logs\")\n",
    "tb_logger = pytorch_lightning.loggers.TensorBoardLogger(save_dir=log_dir)\n",
    "\n",
    "# initialise Lightning's trainer.\n",
    "trainer = pytorch_lightning.Trainer(\n",
    "    max_epochs=600,\n",
    "    logger=tb_logger,\n",
    "    enable_checkpointing=True,\n",
    "    num_sanity_val_steps=1,\n",
    "    log_every_n_steps=16, \n",
    ")\n",
    "\n",
    "trainer.fit(net)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-20T20:38:16.058786028Z",
     "start_time": "2024-03-20T20:38:10.906064298Z"
    }
   },
   "id": "1bb2cdd310a03660"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3a15cd59577d5769"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
